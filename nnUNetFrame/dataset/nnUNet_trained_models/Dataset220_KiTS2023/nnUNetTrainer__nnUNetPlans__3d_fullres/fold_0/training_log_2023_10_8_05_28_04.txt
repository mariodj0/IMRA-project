
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [417.0, 512.0, 512.0], 'spacing': [1.0, 0.78125, 0.78125], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'num_pool_per_axis': [5, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset220_KiTS2023', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [3.0, 0.78125, 0.78125], 'original_median_shape_after_transp': [104, 512, 512], 'image_reader_writer': 'NibabelIOWithReorient', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 3071.0, 'mean': 103.13614654541016, 'median': 102.0, 'min': -1021.0, 'percentile_00_5': -58.0, 'percentile_99_5': 302.0, 'std': 73.3431396484375}}} 
 
2023-10-08 05:28:20.424692: unpacking dataset... 
2023-10-08 05:33:30.543817: unpacking done... 
2023-10-08 05:33:31.358954: do_dummy_2d_data_aug: False 
2023-10-08 05:33:31.373523: Using splits from existing split file: /content/drive/MyDrive/nnUNetFrame/dataset/nnUNet_preprocessed/Dataset220_KiTS2023/splits_final.json 
2023-10-08 05:33:31.974288: The split file contains 5 splits. 
2023-10-08 05:33:31.979076: Desired fold for training: 0 
2023-10-08 05:33:31.983585: This split has 120 training and 30 validation cases. 
2023-10-08 05:33:49.289213:  
2023-10-08 05:33:49.291712: Epoch 50 
2023-10-08 05:33:49.299859: Current learning rate: 0.00955 
2023-10-08 05:43:57.005505: train_loss -0.5762 
2023-10-08 05:43:57.691926: val_loss -0.5852 
2023-10-08 05:43:57.696483: Pseudo dice [0.8817, 0.7445, 0.7282] 
2023-10-08 05:43:57.700523: Epoch time: 607.72 s 
2023-10-08 05:44:00.205291:  
2023-10-08 05:44:00.207830: Epoch 51 
2023-10-08 05:44:00.210526: Current learning rate: 0.00954 
2023-10-08 05:51:29.687121: train_loss -0.5756 
2023-10-08 05:51:30.306911: val_loss -0.6213 
2023-10-08 05:51:30.309459: Pseudo dice [0.8916, 0.7072, 0.6938] 
2023-10-08 05:51:30.311635: Epoch time: 449.48 s 
2023-10-08 05:51:32.958703:  
2023-10-08 05:51:32.986209: Epoch 52 
2023-10-08 05:51:32.993277: Current learning rate: 0.00953 
2023-10-08 05:59:16.173618: train_loss -0.5898 
2023-10-08 05:59:16.996217: val_loss -0.5625 
2023-10-08 05:59:17.001126: Pseudo dice [0.8722, 0.7471, 0.7387] 
2023-10-08 05:59:17.005222: Epoch time: 463.21 s 
2023-10-08 05:59:18.782444:  
2023-10-08 05:59:18.785100: Epoch 53 
2023-10-08 05:59:18.787832: Current learning rate: 0.00952 
2023-10-08 06:07:01.563449: train_loss -0.5744 
2023-10-08 06:07:02.218333: val_loss -0.5036 
2023-10-08 06:07:02.479355: Pseudo dice [0.8393, 0.7448, 0.7424] 
2023-10-08 06:07:02.666290: Epoch time: 462.78 s 
2023-10-08 06:07:02.674006: Yayy! New best EMA pseudo Dice: 0.7534 
2023-10-08 06:07:08.055352:  
2023-10-08 06:07:08.057775: Epoch 54 
2023-10-08 06:07:08.059983: Current learning rate: 0.00951 
2023-10-08 06:14:25.746297: train_loss -0.5273 
2023-10-08 06:14:26.892733: val_loss -0.6425 
2023-10-08 06:14:26.896721: Pseudo dice [0.8986, 0.7818, 0.7794] 
2023-10-08 06:14:26.898501: Epoch time: 437.69 s 
2023-10-08 06:14:26.900176: Yayy! New best EMA pseudo Dice: 0.7601 
2023-10-08 06:14:33.428037:  
2023-10-08 06:14:33.431149: Epoch 55 
2023-10-08 06:14:33.434750: Current learning rate: 0.0095 
2023-10-08 06:21:36.421435: train_loss -0.6065 
2023-10-08 06:21:36.998586: val_loss -0.508 
2023-10-08 06:21:37.004924: Pseudo dice [0.8673, 0.6842, 0.6387] 
2023-10-08 06:21:37.007978: Epoch time: 422.99 s 
2023-10-08 06:21:39.758242:  
2023-10-08 06:21:39.761076: Epoch 56 
2023-10-08 06:21:39.763824: Current learning rate: 0.00949 
2023-10-08 06:29:13.323210: train_loss -0.6054 
2023-10-08 06:29:13.960822: val_loss -0.5788 
2023-10-08 06:29:13.978480: Pseudo dice [0.9087, 0.7829, 0.7653] 
2023-10-08 06:29:14.009147: Epoch time: 453.56 s 
2023-10-08 06:29:14.036405: Yayy! New best EMA pseudo Dice: 0.7633 
2023-10-08 06:29:21.241063:  
2023-10-08 06:29:21.243836: Epoch 57 
2023-10-08 06:29:21.247066: Current learning rate: 0.00949 
2023-10-08 06:37:11.872694: train_loss -0.5771 
2023-10-08 06:37:13.492916: val_loss -0.5892 
2023-10-08 06:37:13.501657: Pseudo dice [0.886, 0.7582, 0.7526] 
2023-10-08 06:37:13.506906: Epoch time: 470.63 s 
2023-10-08 06:37:13.511943: Yayy! New best EMA pseudo Dice: 0.7668 
2023-10-08 06:37:19.663797:  
2023-10-08 06:37:19.666751: Epoch 58 
2023-10-08 06:37:19.669278: Current learning rate: 0.00948 
2023-10-08 06:44:13.095559: train_loss -0.5473 
2023-10-08 06:44:13.908771: val_loss -0.5412 
2023-10-08 06:44:14.049756: Pseudo dice [0.8999, 0.6926, 0.6746] 
2023-10-08 06:44:14.072352: Epoch time: 413.43 s 
2023-10-08 06:44:18.898072:  
2023-10-08 06:44:21.331458: Epoch 59 
2023-10-08 06:44:21.344340: Current learning rate: 0.00947 
2023-10-08 06:51:55.314411: train_loss -0.6148 
2023-10-08 06:51:56.221703: val_loss -0.5677 
2023-10-08 06:51:56.235800: Pseudo dice [0.912, 0.7648, 0.7392] 
2023-10-08 06:51:56.248513: Epoch time: 456.42 s 
2023-10-08 06:51:56.263190: Yayy! New best EMA pseudo Dice: 0.7697 
2023-10-08 06:52:01.983939:  
2023-10-08 06:52:01.987020: Epoch 60 
2023-10-08 06:52:01.989688: Current learning rate: 0.00946 
2023-10-08 06:59:41.643394: train_loss -0.6479 
2023-10-08 06:59:43.255025: val_loss -0.5988 
2023-10-08 06:59:43.261976: Pseudo dice [0.9148, 0.7794, 0.78] 
2023-10-08 06:59:43.264632: Epoch time: 459.65 s 
2023-10-08 06:59:43.267281: Yayy! New best EMA pseudo Dice: 0.7752 
2023-10-08 06:59:48.250880:  
2023-10-08 06:59:48.253278: Epoch 61 
2023-10-08 06:59:48.255621: Current learning rate: 0.00945 
2023-10-08 07:07:25.396690: train_loss -0.5562 
2023-10-08 07:07:25.974991: val_loss -0.6159 
2023-10-08 07:07:25.994185: Pseudo dice [0.913, 0.8076, 0.7888] 
2023-10-08 07:07:29.431169: Epoch time: 457.14 s 
2023-10-08 07:07:29.463613: Yayy! New best EMA pseudo Dice: 0.7813 
2023-10-08 07:07:33.479330:  
2023-10-08 07:07:33.482332: Epoch 62 
2023-10-08 07:07:33.485302: Current learning rate: 0.00944 
2023-10-08 07:14:59.584754: train_loss -0.5833 
2023-10-08 07:15:00.200772: val_loss -0.6318 
2023-10-08 07:15:00.464243: Pseudo dice [0.8948, 0.7723, 0.7669] 
2023-10-08 07:15:02.244584: Epoch time: 446.1 s 
2023-10-08 07:15:02.338671: Yayy! New best EMA pseudo Dice: 0.7843 
2023-10-08 07:15:05.655053:  
2023-10-08 07:15:05.657569: Epoch 63 
2023-10-08 07:15:05.660657: Current learning rate: 0.00943 
2023-10-08 07:22:18.340645: train_loss -0.6185 
2023-10-08 07:22:19.660848: val_loss -0.4849 
2023-10-08 07:22:19.818260: Pseudo dice [0.8564, 0.701, 0.6909] 
2023-10-08 07:22:19.932453: Epoch time: 432.69 s 
2023-10-08 07:22:25.508062:  
2023-10-08 07:22:25.525783: Epoch 64 
2023-10-08 07:22:25.535954: Current learning rate: 0.00942 
2023-10-08 07:30:06.746294: train_loss -0.5857 
2023-10-08 07:30:07.359144: val_loss -0.6482 
2023-10-08 07:30:07.362082: Pseudo dice [0.9285, 0.8111, 0.7836] 
2023-10-08 07:30:07.364240: Epoch time: 461.23 s 
2023-10-08 07:30:07.368248: Yayy! New best EMA pseudo Dice: 0.7869 
2023-10-08 07:30:18.582769:  
2023-10-08 07:30:18.585955: Epoch 65 
2023-10-08 07:30:18.589820: Current learning rate: 0.00941 
2023-10-08 07:37:27.472794: train_loss -0.5962 
2023-10-08 07:37:28.612479: val_loss -0.4514 
2023-10-08 07:37:28.623847: Pseudo dice [0.8679, 0.6306, 0.5896] 
2023-10-08 07:37:28.628404: Epoch time: 428.88 s 
2023-10-08 07:37:31.514325:  
2023-10-08 07:37:32.189747: Epoch 66 
2023-10-08 07:37:32.804031: Current learning rate: 0.0094 
2023-10-08 07:45:05.678950: train_loss -0.6117 
2023-10-08 07:45:06.329069: val_loss -0.6271 
2023-10-08 07:45:06.337908: Pseudo dice [0.9303, 0.7643, 0.7583] 
2023-10-08 07:45:06.343564: Epoch time: 454.17 s 
2023-10-08 07:45:12.291202:  
2023-10-08 07:45:12.518180: Epoch 67 
2023-10-08 07:45:12.642323: Current learning rate: 0.00939 
2023-10-08 07:52:34.667869: train_loss -0.6024 
2023-10-08 07:52:35.465600: val_loss -0.5518 
2023-10-08 07:52:35.481065: Pseudo dice [0.8795, 0.6987, 0.6788] 
2023-10-08 07:52:35.490988: Epoch time: 442.37 s 
2023-10-08 07:52:38.001511:  
2023-10-08 07:52:38.066688: Epoch 68 
2023-10-08 07:52:38.070093: Current learning rate: 0.00939 
2023-10-08 07:59:54.044741: train_loss -0.6055 
2023-10-08 07:59:54.645057: val_loss -0.6822 
2023-10-08 07:59:54.648767: Pseudo dice [0.9296, 0.8645, 0.8707] 
2023-10-08 07:59:54.651978: Epoch time: 436.04 s 
2023-10-08 07:59:54.654844: Yayy! New best EMA pseudo Dice: 0.7898 
2023-10-08 07:59:59.397353:  
2023-10-08 07:59:59.400088: Epoch 69 
2023-10-08 07:59:59.404442: Current learning rate: 0.00938 
2023-10-08 08:07:35.910172: train_loss -0.6603 
2023-10-08 08:07:36.498411: val_loss -0.6237 
2023-10-08 08:07:36.522264: Pseudo dice [0.9202, 0.7768, 0.7703] 
2023-10-08 08:07:36.525940: Epoch time: 456.51 s 
2023-10-08 08:07:36.528804: Yayy! New best EMA pseudo Dice: 0.793 
2023-10-08 08:07:40.317524:  
2023-10-08 08:07:40.353630: Epoch 70 
2023-10-08 08:07:40.360881: Current learning rate: 0.00937 
2023-10-08 08:14:42.894920: train_loss -0.5814 
2023-10-08 08:14:43.527536: val_loss -0.5707 
2023-10-08 08:14:43.566999: Pseudo dice [0.9004, 0.8217, 0.8033] 
2023-10-08 08:14:43.571301: Epoch time: 422.57 s 
2023-10-08 08:14:43.578817: Yayy! New best EMA pseudo Dice: 0.7979 
2023-10-08 08:14:47.928936:  
2023-10-08 08:14:47.932849: Epoch 71 
2023-10-08 08:14:47.936176: Current learning rate: 0.00936 
2023-10-08 08:22:22.630630: train_loss -0.58 
2023-10-08 08:22:23.597826: val_loss -0.6938 
2023-10-08 08:22:23.606908: Pseudo dice [0.9124, 0.8424, 0.8392] 
2023-10-08 08:22:23.609616: Epoch time: 454.7 s 
2023-10-08 08:22:23.612056: Yayy! New best EMA pseudo Dice: 0.8046 
2023-10-08 08:22:27.079474:  
2023-10-08 08:22:27.082726: Epoch 72 
2023-10-08 08:22:28.110346: Current learning rate: 0.00935 
2023-10-08 08:29:23.880218: train_loss -0.6338 
2023-10-08 08:29:24.903999: val_loss -0.6497 
2023-10-08 08:29:24.915462: Pseudo dice [0.9035, 0.8512, 0.8482] 
2023-10-08 08:29:24.920830: Epoch time: 416.79 s 
2023-10-08 08:29:24.926029: Yayy! New best EMA pseudo Dice: 0.8109 
2023-10-08 08:29:28.272464:  
2023-10-08 08:29:28.275548: Epoch 73 
2023-10-08 08:29:28.278433: Current learning rate: 0.00934 
2023-10-08 08:37:27.509899: train_loss -0.6453 
2023-10-08 08:37:28.113822: val_loss -0.6067 
2023-10-08 08:37:28.116872: Pseudo dice [0.9168, 0.8107, 0.8107] 
2023-10-08 08:37:28.119403: Epoch time: 479.23 s 
2023-10-08 08:37:28.121917: Yayy! New best EMA pseudo Dice: 0.8144 
2023-10-08 08:37:31.985998:  
2023-10-08 08:37:31.988719: Epoch 74 
2023-10-08 08:37:31.991096: Current learning rate: 0.00933 
2023-10-08 08:45:04.710603: train_loss -0.6756 
2023-10-08 08:45:06.500096: val_loss -0.6664 
2023-10-08 08:45:06.530832: Pseudo dice [0.9338, 0.8425, 0.8308] 
2023-10-08 08:45:06.539928: Epoch time: 452.72 s 
2023-10-08 08:45:06.548771: Yayy! New best EMA pseudo Dice: 0.8199 
2023-10-08 08:45:15.575398:  
2023-10-08 08:45:15.578348: Epoch 75 
2023-10-08 08:45:15.580970: Current learning rate: 0.00932 
2023-10-08 08:52:31.819083: train_loss -0.6697 
2023-10-08 08:52:32.766036: val_loss -0.6346 
2023-10-08 08:52:32.800000: Pseudo dice [0.9085, 0.8705, 0.8657] 
2023-10-08 08:52:32.816727: Epoch time: 436.24 s 
2023-10-08 08:52:32.825127: Yayy! New best EMA pseudo Dice: 0.826 
2023-10-08 08:52:42.327212:  
2023-10-08 08:52:43.590883: Epoch 76 
2023-10-08 08:52:43.593931: Current learning rate: 0.00931 
2023-10-08 08:59:51.972312: train_loss -0.6372 
2023-10-08 08:59:52.580147: val_loss -0.6918 
2023-10-08 08:59:52.591221: Pseudo dice [0.9336, 0.8326, 0.8105] 
2023-10-08 08:59:52.596515: Epoch time: 429.64 s 
2023-10-08 08:59:52.602581: Yayy! New best EMA pseudo Dice: 0.8293 
2023-10-08 08:59:57.765645:  
2023-10-08 08:59:57.768349: Epoch 77 
2023-10-08 08:59:57.771212: Current learning rate: 0.0093 
2023-10-08 09:07:57.302491: train_loss -0.5717 
2023-10-08 09:07:58.979989: val_loss -0.5379 
2023-10-08 09:07:58.990729: Pseudo dice [0.8939, 0.7017, 0.6903] 
2023-10-08 09:07:58.996906: Epoch time: 479.53 s 
2023-10-08 09:08:01.717573:  
2023-10-08 09:08:01.720070: Epoch 78 
2023-10-08 09:08:01.722515: Current learning rate: 0.0093 
2023-10-08 09:15:09.420477: train_loss -0.6028 
2023-10-08 09:15:10.007742: val_loss -0.2361 
2023-10-08 09:15:10.017402: Pseudo dice [0.7362, 0.288, 0.2728] 
2023-10-08 09:15:10.020150: Epoch time: 427.7 s 
2023-10-08 09:15:13.011223:  
2023-10-08 09:15:13.013724: Epoch 79 
2023-10-08 09:15:13.015435: Current learning rate: 0.00929 
2023-10-08 09:22:47.535546: train_loss -0.5956 
2023-10-08 09:22:48.186849: val_loss -0.663 
2023-10-08 09:22:48.200815: Pseudo dice [0.9195, 0.8103, 0.791] 
2023-10-08 09:22:48.204506: Epoch time: 454.52 s 
2023-10-08 09:22:53.882653:  
2023-10-08 09:22:53.921913: Epoch 80 
2023-10-08 09:22:54.080630: Current learning rate: 0.00928 
